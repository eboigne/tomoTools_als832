{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:90% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/e/eboigne/tomoTools_als832/tomoTools\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import ipywidgets\n",
    "ipywidgets.Widget.close_all()\n",
    "\n",
    "import SimpleITK\n",
    "import datetime\n",
    "import gc # Garbage collected\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import time\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename, askopenfilenames, askdirectory\n",
    "import torch\n",
    "import skimage\n",
    "from skimage import measure\n",
    "from skimage.morphology import skeletonize\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import sknw # https://github.com/Image-Py/sknw\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "this_path = os.getcwd()\n",
    "print(this_path)\n",
    "\n",
    "\n",
    "import h5py\n",
    "# import File\n",
    "from scripts import *\n",
    "import wrapper_ASTRA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01', '02', '03', '04a', '04b', '05a', '05b', '06a', '06b', '07a', '07b', '08a', '08b', '09a', '09b', '10a', '10b', '11', '12', '13a', '13b', '14a', '14b', '15a', '15b', '16a', '16b', '17a', '17b', '18a', '18b', '19a', '19b', '20a', '20b', '21a', '21b', '22', '23', '24']\n"
     ]
    }
   ],
   "source": [
    "path_save = '/global/homes/e/eboigne/cfs_als/2022_wood/'\n",
    "\n",
    "# case = 'run21_oak_lowHeat'\n",
    "case = 'run23_walnut_lowHeat'\n",
    "# case = 'run24_birch_lowHeat'\n",
    "# case = 'run26_birch_highHeat'\n",
    "\n",
    "voxel_size = 3.24*2 # [microns]\n",
    "\n",
    "list_cases_folder_name = sorted([e for e in os.listdir(path_save+case) if not 'probe' in e and not '.tif' in e and not '.pickle' in e])\n",
    "run = case[3:5]+'_Sample'\n",
    "list_cases_h5 = sorted([e for e in os.listdir(path_save) if '.h5' in e and run in e])\n",
    "\n",
    "print(list_cases_folder_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% WHich case to look at?\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 01\n",
      "1 02\n",
      "2 03\n",
      "3 04a\n",
      "4 04b\n",
      "5 05a\n",
      "6 05b\n",
      "7 06a\n",
      "8 06b\n",
      "9 07a\n",
      "10 07b\n",
      "11 08a\n",
      "12 08b\n",
      "13 09a\n",
      "14 09b\n",
      "15 10a\n",
      "16 10b\n",
      "17 11\n",
      "18 12\n",
      "19 13a\n",
      "20 13b\n",
      "21 14a\n",
      "22 14b\n",
      "23 15a\n",
      "24 15b\n",
      "25 16a\n",
      "26 16b\n",
      "27 17a\n",
      "28 17b\n",
      "29 18a\n",
      "30 18b\n",
      "31 19a\n",
      "32 19b\n",
      "33 20a\n",
      "34 20b\n",
      "35 21a\n",
      "36 21b\n",
      "37 22\n",
      "38 23\n",
      "39 24\n",
      "(40, 1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "ind_slice = 250\n",
    "\n",
    "slices = []\n",
    "for ind_case, this_case in enumerate(list_cases_folder_name[:]):\n",
    "    slices.append(File(path_save+case+'/'+this_case+'/b_movingRegisteredToStatic/').read(ind_slice))\n",
    "    print(ind_case, this_case)\n",
    "\n",
    "slices = np.array(slices)\n",
    "print(slices.shape)\n",
    "File(path_save+case+'/probe_slice_'+str(ind_slice).zfill(4)).saveTiffStack(slices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Probe one slice\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (0, '01') c 56073.0\n",
      "2 (1, '02') c 56373.0\n",
      "3 (2, '03') c 56695.0\n",
      "5 (3, '04') a/b 57252.0 57278.26\n",
      "7 (4, '05') a/b 57448.0 57474.26\n",
      "9 (5, '06') a/b 57629.0 57655.26\n",
      "11 (6, '07') a/b 57811.0 57837.26\n",
      "13 (7, '08') a/b 57997.0 58023.26\n",
      "15 (8, '09') a/b 58182.0 58208.26\n",
      "17 (9, '10') a/b 58369.0 58395.26\n",
      "18 (10, '11') c 58550.0\n",
      "19 (11, '12') c 58732.0\n",
      "21 (12, '13') a/b 58916.0 58942.26\n",
      "23 (13, '14') a/b 59100.0 59126.26\n",
      "25 (14, '15') a/b 59282.0 59308.26\n",
      "27 (15, '16') a/b 59467.0 59493.26\n",
      "29 (16, '17') a/b 59653.0 59679.26\n",
      "31 (17, '18') a/b 59838.0 59864.26\n",
      "33 (18, '19') a/b 60027.0 60053.26\n",
      "35 (19, '20') a/b 60220.0 60246.26\n",
      "37 (20, '21') a/b 60403.0 60429.26\n",
      "38 (21, '22') c 60585.0\n",
      "39 (22, '23') c 60889.0\n",
      "40 (23, '24') c 61198.0\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "for this_case in enumerate(list_cases_h5):\n",
    "    c.append(this_case[1][35:37])\n",
    "\n",
    "t = []\n",
    "\n",
    "list_scans = []\n",
    "\n",
    "for this_case in enumerate(c):\n",
    "    if str(this_case[1]) in list_cases_folder_name or '-post' in [e for e in list_cases_folder_name if this_case[1] in e][0]:\n",
    "        cc = list_cases_h5[this_case[0]][9:15]\n",
    "        h = cc[0:2]\n",
    "        m = cc[2:4]\n",
    "        s = cc[4:]\n",
    "        ts = float(int(h)*3600+int(m)*60+int(s))\n",
    "        t.append(ts) #c\n",
    "        print(len(t), this_case, 'c', ts)\n",
    "    else:\n",
    "        cc = list_cases_h5[this_case[0]][9:15]\n",
    "        h = cc[0:2]\n",
    "        m = cc[2:4]\n",
    "        s = cc[4:]\n",
    "        ts0 = float(int(h)*3600+int(m)*60+int(s))\n",
    "        t.append(ts0) #a\n",
    "        ts1 = 26.26 + ts0\n",
    "        t.append(ts1) #b\n",
    "        print(len(t), this_case, 'a/b', ts0 ,ts1)\n",
    "\n",
    "scan_times = np.array(t) - t[0]\n",
    "\n",
    "\n",
    "for ind_scan,scan_time in enumerate(scan_times):\n",
    "    this_scan = Data()\n",
    "    # print(ind_scan)\n",
    "    this_scan.scan_time = scan_time\n",
    "    this_scan.folder_name = list_cases_folder_name[ind_scan]\n",
    "    this_scan.full_path = path_save+case+'/'+this_scan.folder_name\n",
    "    this_scan.ind = ind_scan\n",
    "    list_scans.append(this_scan)\n",
    "\n",
    "assert(len(list_cases_folder_name) ==len(scan_times)), 'Wrong sizes'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Setup times and scan list\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01', '02', '03', '04a', '04b', '05a', '05b', '06a', '06b', '07a', '07b', '08a', '08b', '09a', '09b', '10a', '10b', '11', '12', '13a', '13b', '14a', '14b', '15a', '15b', '16a', '16b', '17a', '17b', '18a', '18b', '19a', '19b', '20a', '20b', '21a', '21b', '22', '23', '24']\n"
     ]
    }
   ],
   "source": [
    "print(list_cases_folder_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def compute_height(properties):\n",
    "    for prop in properties:\n",
    "        bbox = prop.bbox\n",
    "        prop.height = bbox[3]-bbox[0] # Along z, in pixels\n",
    "    return(properties)\n",
    "\n",
    "def compute_equivalent_cylinder_diameters(properties):\n",
    "    for prop in properties:\n",
    "        equivalent_cylinder_cross_section = prop.area / prop.height\n",
    "        prop.equivalent_cylinder_diameter = np.sqrt(4*equivalent_cylinder_cross_section/np.pi) * voxel_size\n",
    "    return(properties)\n",
    "\n",
    "def get_path_length(graph, path):\n",
    "    this_sum=0\n",
    "    for i in range(len(path)-1):\n",
    "        this_sum += graph.get_edge_data(path[i], path[i+1])['weight']\n",
    "    return(this_sum)\n",
    "\n",
    "def longest_simple_path(graph, source, target):\n",
    "    longest_path = None\n",
    "    longest_path_length = 0\n",
    "    for path in nx.all_simple_paths(graph, source=source, target=target):\n",
    "        path_length = get_path_length(graph,path)\n",
    "        if path_length > longest_path_length:\n",
    "            longest_path_length = path_length\n",
    "            longest_path = path\n",
    "    return longest_path, longest_path_length\n",
    "\n",
    "def find_longest_subgraph_from_ends(graph, bbox):\n",
    "\n",
    "    ind_end_nodes = [x for x in graph.nodes if graph.degree(x) == 1]\n",
    "    ind_end_bottom = [x for x in ind_end_nodes if graph.nodes[x]['o'][0] - bbox[0] <=  bbox[3] - graph.nodes[x]['o'][0]]\n",
    "    ind_end_top = [x for x in ind_end_nodes if not x in ind_end_bottom]\n",
    "\n",
    "    longest_path_subgraph = None\n",
    "    longest_path_length = 0\n",
    "    longest_path = []\n",
    "    for ind_bot in ind_end_bottom:\n",
    "        for ind_top in ind_end_top:\n",
    "            this_longest_path, this_longest_path_length = longest_simple_path(graph, source=ind_bot, target=ind_top)\n",
    "            this_longest_path_subgraph = graph.subgraph(this_longest_path)\n",
    "\n",
    "            if this_longest_path_length > longest_path_length:\n",
    "                longest_path_length = this_longest_path_length\n",
    "                longest_path = this_longest_path\n",
    "                longest_path_subgraph = this_longest_path_subgraph\n",
    "\n",
    "    return(longest_path_subgraph, longest_path, longest_path_length)\n",
    "\n",
    "def compute_tortuosity(skeleton_properties):\n",
    "\n",
    "    for ind_pore in range(len(skeleton_properties)):\n",
    "        bbox = skeleton_properties[ind_pore].bbox\n",
    "        graph = sknw.build_sknw(skeleton_properties[ind_pore].image, multi=False, iso=False, ring=False, full=True)\n",
    "        longest_path_subgraph, longest_path, longest_path_length = find_longest_subgraph_from_ends(graph, bbox)\n",
    "\n",
    "        if longest_path_length == 0:\n",
    "            longest_path_length = np.nan\n",
    "\n",
    "        skeleton_properties[ind_pore].length_skeleton = longest_path_length\n",
    "        skeleton_properties[ind_pore].longest_path = longest_path\n",
    "        # skeleton_properties[ind_pore].longest_path_subgraph = longest_path_subgraph\n",
    "        skeleton_properties[ind_pore].tortuosity = longest_path_length / skeleton_properties[ind_pore].height\n",
    "\n",
    "        # print(ind_pore, longest_path_length, skeleton_properties[ind_pore].tortuosity)\n",
    "    return(skeleton_properties)\n",
    "\n",
    "def compute_skeleton(this_scan, mask_pores, force_redo = False):\n",
    "\n",
    "    if force_redo or not 'b_movingRegisteredToStatic_maskPores_skeleton' in os.listdir(this_scan.full_path):\n",
    "        skeleton = skeletonize(mask_pores)\n",
    "        File(this_scan.full_path+'/b_movingRegisteredToStatic_maskPores_skeleton').saveTiffStack(skeleton, type = 'bool')\n",
    "    else:\n",
    "        print('\\tRe-loading skeleton')\n",
    "        skeleton = File(this_scan.full_path+'/b_movingRegisteredToStatic_maskPores_skeleton').readAll()\n",
    "    return(skeleton)\n",
    "\n",
    "def find_top_bottom_slices_ind(this_scan, mask_solid, ind_all_top, ind_all_bottom):\n",
    "\n",
    "    this_scan.ind_all_margin = 100\n",
    "    this_scan.ind_margin = 30\n",
    "    this_scan.relative_porosity_threshold = 0.15\n",
    "\n",
    "    mask_solid_slice_sum = np.sum(np.sum(mask_solid, axis = 2), axis = 1)\n",
    "    mean_area_ind_all = np.mean(mask_solid_slice_sum[ind_all_top+this_scan.ind_all_margin:ind_all_bottom-this_scan.ind_all_margin])\n",
    "\n",
    "    this_scan.relative_porosity = np.abs(mean_area_ind_all-mask_solid_slice_sum)/mean_area_ind_all\n",
    "\n",
    "    this_scan.ind_top = np.min(np.where(this_scan.relative_porosity<this_scan.relative_porosity_threshold)) + this_scan.ind_margin\n",
    "    this_scan.ind_bottom = np.max(np.where(this_scan.relative_porosity<this_scan.relative_porosity_threshold)) - this_scan.ind_margin\n",
    "\n",
    "    this_scan.height_ROI = this_scan.ind_bottom-this_scan.ind_top + 1\n",
    "\n",
    "    print('\\t\\tDetected ROI:\\tind_top: '+str(this_scan.ind_top)+', ind_bottom: '+str(this_scan.ind_bottom)+', height: '+str(this_scan.height_ROI))\n",
    "    if (this_scan.height_ROI < 200):\n",
    "        print('\\t\\t\\t\\t/!\\ WARNING: small height of ROI detected: check the cropping of mask solid')\n",
    "\n",
    "    return(this_scan)\n",
    "\n",
    "def mask_crop_ROI(mask, ind_top, ind_bottom):\n",
    "    mask[:ind_top] = False\n",
    "    mask[ind_bottom:] = False\n",
    "    return(mask)\n",
    "\n",
    "def close_mask_solid(this_scan, mask_solid, mask_needle, force_redo = False):\n",
    "\n",
    "    if force_redo or not 'b_movingRegisteredToStatic_maskSolidClosed' in os.listdir(this_scan.full_path):\n",
    "\n",
    "        # Option #1: Morphological closing to close the pores\n",
    "        # chunk_size = (500, 500, 500)\n",
    "        # filter_half_width = 12\n",
    "        # structure = custom_3d_kernel_sphere(filter_half_width)\n",
    "        # mask_solid_dilated = apply_3d_image_processing_on_subvolumes(mask_solid, fast_pytorch_mask_dilation, chunk_size_max = chunk_size, overlap = filter_half_width, radius = filter_half_width)\n",
    "        # mask_solid_closed = ~apply_3d_image_processing_on_subvolumes(~mask_solid_dilated, fast_pytorch_mask_dilation, chunk_size_max = chunk_size, overlap = filter_half_width, radius = filter_half_width)\n",
    "\n",
    "        # Option #2: Fill holes algorithm run slice by slice\n",
    "        n_threads = 16\n",
    "        mask_solid_closed = np.zeros_like(mask_solid)\n",
    "        with multiprocessing.Pool(n_threads) as p:\n",
    "            mask_solid_closed = np.array(p.map(scipy.ndimage.morphology.binary_fill_holes, mask_solid))\n",
    "\n",
    "        # 1 core version of option #2\n",
    "        # for ind, slice in enumerate(mask_solid):\n",
    "        #     mask_solid_closed[ind] = scipy.ndimage.morphology.binary_fill_holes(slice)\n",
    "        #     print(ind)\n",
    "\n",
    "        mask_solid_closed[mask_needle] = False\n",
    "        File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolidClosed').saveTiffStack(mask_solid_closed, type = 'bool')\n",
    "    else:\n",
    "        print('\\tRe-loading mask solid closed')\n",
    "        mask_solid_closed = File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolidClosed').readAll()\n",
    "\n",
    "    return(mask_solid_closed.astype('bool'))\n",
    "\n",
    "def rotate_data(data, rot_coronal_Fiji, rot_sagittal_Fiji, rot_z_Fiji = 0.0):\n",
    "    pad = 1\n",
    "    transform_rotation = sitk.Euler3DTransform()\n",
    "    center_xyz = (data.shape[0] / 2.0+pad, data.shape[1] / 2.0+pad, data.shape[2] / 2.0+pad) # TODO: Is this correct? Not consistent with below\n",
    "    center_angle = 0.0\n",
    "    offsets_xyz = (0.0, 0.0, 0.0)\n",
    "\n",
    "    transform_rotation.SetFixedParameters((center_xyz[0], center_xyz[1], center_xyz[2], center_angle))\n",
    "    eulerAngle_xyz_deg = (-rot_sagittal_Fiji, rot_coronal_Fiji, 0.0)\n",
    "    transform_rotation.SetParameters((eulerAngle_xyz_deg[0]/180.0*np.pi, eulerAngle_xyz_deg[1]/180.0*np.pi, eulerAngle_xyz_deg[2]/180.0*np.pi, offsets_xyz[0], offsets_xyz[1], offsets_xyz[2]))\n",
    "\n",
    "    data_bigger = np.zeros([data.shape[0]+pad*2, data.shape[1]+pad*2, data.shape[2]+pad*2])\n",
    "    data_bigger[pad:-pad, pad:-pad, pad:-pad] = data\n",
    "    data_out = applyTransformToVolume(data_bigger, data_bigger, transform_rotation)\n",
    "\n",
    "    # Then only axial rotation. Can't do all axis at once to match Fiji\n",
    "    if rot_z_Fiji != 0:\n",
    "        center_xyz = (data_out.shape[2] / 2.0, data_out.shape[1] / 2.0, data_out.shape[0] / 2.0)\n",
    "        transform_rotation.SetFixedParameters((center_xyz[0], center_xyz[1], center_xyz[2], center_angle))\n",
    "        eulerAngle_xyz_deg = (0.0, 0.0, -rot_z_Fiji)\n",
    "        offsets_xyz2 = (0.0, -50.0, 0.0)\n",
    "        transform_rotation.SetParameters((eulerAngle_xyz_deg[0]/180.0*np.pi, eulerAngle_xyz_deg[1]/180.0*np.pi, eulerAngle_xyz_deg[2]/180.0*np.pi, offsets_xyz2[0], offsets_xyz2[1], offsets_xyz2[2]))\n",
    "        data_out2 = applyTransformToVolume(data_out, data_out, transform_rotation)\n",
    "    else:\n",
    "        data_out2 = data_out\n",
    "\n",
    "    return(data_out2[pad:-pad, pad:-pad, pad:-pad])\n",
    "\n",
    "def analyze_pores_2d(this_scan, mask_pores):\n",
    "    tic = time.time()\n",
    "\n",
    "    this_scan.slice_angles_to_growth = []\n",
    "    this_scan.slice_ellipse_major_diameters = []\n",
    "    this_scan.slice_ellipse_minor_diameters = []\n",
    "    this_scan.slice_ellipse_eccentricity = []\n",
    "    this_scan.slice_areas = []\n",
    "    this_scan.slice_eq_diameters = []\n",
    "    this_scan.slice_perimeters = []\n",
    "\n",
    "    for ind, slice in enumerate(mask_pores):\n",
    "        slice_lab = measure.label(slice)\n",
    "        slice_properties = np.array(measure.regionprops(slice_lab))\n",
    "\n",
    "         # Angle w.r.t vertical (growth axis), with positive being CCW, in degrees, from -90 to 90\n",
    "        angles = 180 / np.pi * np.array([p.orientation for p in slice_properties]) # From -90 to 90\n",
    "        # angles[angles<0] = 180+angles[angles<0]\n",
    "        this_scan.slice_angles_to_growth.append(angles)\n",
    "\n",
    "        this_scan.slice_ellipse_major_diameters.append(np.array([p.axis_major_length for p in slice_properties]))\n",
    "        this_scan.slice_ellipse_minor_diameters.append(np.array([p.axis_minor_length for p in slice_properties]))\n",
    "        this_scan.slice_ellipse_eccentricity.append(np.array([p.eccentricity for p in slice_properties]))\n",
    "        this_scan.slice_eq_diameters.append(np.array([p.equivalent_diameter_area for p in slice_properties]))\n",
    "        this_scan.slice_perimeters.append(np.array([p.perimeter for p in slice_properties]))\n",
    "        this_scan.slice_areas.append(np.array([p.area for p in slice_properties]))\n",
    "\n",
    "        if ind % 100 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "    return(this_scan)\n",
    "\n",
    "def mean_attenuation_volume(this_scan, mask_to_use, data):\n",
    "    mu_s = np.nanmean(data[mask_to_use])\n",
    "    V_s = np.nansum(mask_to_use.flatten())\n",
    "    V_s_m = np.nansum(mask_to_use.flatten())*(voxel_size*10**-6)**3 #[m3]\n",
    "    return(mu_s,V_s, V_s_m)\n",
    "\n",
    "def cleanup_keep_single_particle(mask_solid):\n",
    "\n",
    "    # Morphological opening to separate sample from artifact regions\n",
    "    chunk_size = (500, 500, 500)\n",
    "    filter_half_width = 7\n",
    "    structure = custom_3d_kernel_sphere(filter_half_width)\n",
    "    mask_solid_eroded = ~apply_3d_image_processing_on_subvolumes(~mask_solid, fast_pytorch_mask_dilation, chunk_size_max = chunk_size, overlap = filter_half_width, radius = filter_half_width)\n",
    "    mask_solid_opened = apply_3d_image_processing_on_subvolumes(mask_solid_eroded, fast_pytorch_mask_dilation, chunk_size_max = chunk_size, overlap = filter_half_width, radius = filter_half_width)\n",
    "\n",
    "    # Isolate largest connected particle\n",
    "    lab = measure.label(mask_solid_opened)\n",
    "    properties = np.array(measure.regionprops(lab))\n",
    "    areas = np.array([e.area for e in properties])\n",
    "    ind_max_area = np.argmax(areas)\n",
    "    mask_out = lab == properties[ind_max_area].label\n",
    "\n",
    "    return(mask_out)\n",
    "\n",
    "def compute_mask_solid(this_scan, data, threshold, force_redo = False):\n",
    "\n",
    "    if force_redo or not 'b_movingRegisteredToStatic_maskSolid' in os.listdir(this_scan.full_path):\n",
    "\n",
    "        threshold_needle = 4.0 # Linear attenuation threshold [/cm]\n",
    "        mask_growth_needle = 10 # Grow the mask obtained using threshold_needle with this amount of pixels\n",
    "        mask_enclosing_circle = tifffile.imread(path_save+case+'/maskEnclosingCircle.tif') > 0\n",
    "        bin_factor_dilatation = 1 # Binning before smoothing, as an option to speed things up for large kernels\n",
    "        filter_half_width = 3 # After binning (equal to sigma for gaussian filter)\n",
    "\n",
    "        kernel = custom_3d_gaussian_filter(filter_half_width)\n",
    "\n",
    "        if bin_factor_dilatation > 1:\n",
    "            data_smoothed = fast_pytorch_bin_3d(data,bin_factor_dilatation, chunk_size = 71)\n",
    "        else:\n",
    "            data_smoothed = data\n",
    "\n",
    "        print('\\t3D smoothing for thresholding of mask_solid and mask_needle:')\n",
    "        data_smoothed = apply_3d_image_processing_on_subvolumes(data_smoothed, fast_pytorch_convolution, kernel_array = kernel, chunk_size_max = (500, 500, 500), overlap = 3*filter_half_width)\n",
    "\n",
    "        if bin_factor_dilatation > 1:\n",
    "            data_smoothed = skimage.transform.rescale(data_smoothed, bin_factor_dilatation, multichannel=False)\n",
    "\n",
    "        # File(this_scan.full_path+'/b_movingRegisteredToStatic_smoothed').saveTiffStack(data_smoothed)\n",
    "\n",
    "        mask_needle = data_smoothed > threshold_needle\n",
    "        mask_needle = apply_3d_image_processing_on_subvolumes(mask_needle, fast_pytorch_mask_dilation, chunk_size_max = (500, 500, 500), overlap = mask_growth_needle, radius = mask_growth_needle)\n",
    "\n",
    "        mask_solid = data_smoothed > threshold\n",
    "        mask_solid[mask_needle] = False\n",
    "\n",
    "        for ind, slice in enumerate(mask_solid):\n",
    "            slice[~mask_enclosing_circle] = False\n",
    "            mask_solid[ind] = slice\n",
    "\n",
    "        # Computing top and bottom indices for a ROI within the solid\n",
    "        this_scan = find_top_bottom_slices_ind(this_scan, mask_solid, this_scan.ind_all_top, this_scan.ind_all_bottom)\n",
    "\n",
    "        # Close mask_solid, using 2D fill holes algorithm slice by slice\n",
    "        print('\\tClosing mask solid')\n",
    "        mask_solid_closed = close_mask_solid(this_scan, mask_solid, mask_needle, force_redo = force_redo)\n",
    "\n",
    "        # Clean up mask_solid_closed and mask_solid to keep single large particle region, and remove artifacts above and below sample\n",
    "        print('\\tCleaning up mask solid (3D opening), isolating largest particle')\n",
    "        mask_solid_closed = cleanup_keep_single_particle(mask_solid_closed)\n",
    "        mask_solid_closed = mask_crop_ROI(mask_solid_closed, this_scan.ind_top-this_scan.ind_margin-20, this_scan.ind_bottom+this_scan.ind_margin+20)\n",
    "        mask_solid = mask_solid * mask_solid_closed\n",
    "\n",
    "        print('\\tSaving mask_solid and mask_solid_closed:')\n",
    "        File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolid').saveTiffStack(mask_solid, type = 'bool')\n",
    "        File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolidClosed').saveTiffStack(mask_solid_closed, type = 'bool')\n",
    "        # File(this_scan.full_path+'/b_movingRegisteredToStatic_maskNeedle').saveTiffStack(mask_needle, type = 'bool')\n",
    "\n",
    "    else:\n",
    "        print('\\tRe-loading mask_solid and mask_solid_closed')\n",
    "        mask_solid = File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolid').readAll()\n",
    "        mask_solid_closed = File(this_scan.full_path+'/b_movingRegisteredToStatic_maskSolidClosed').readAll()\n",
    "        # mask_needle = File(this_scan.full_path+'/b_movingRegisteredToStatic_maskNeedle').readAll()\n",
    "\n",
    "    return(this_scan, mask_solid.astype('bool'), mask_solid_closed.astype('bool'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Function to compute mask_solid\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Rotation\n",
    "rot_coronal_Fiji = 1.0 # [Deg]\n",
    "rot_sagittal_Fiji = -1.0 # [Deg]\n",
    "rot_z_Fiji = 43.0 # [Deg] This angle: such that tree growth is vertical from bottom to top in Fiji, after sagittal / coronal rotation\n",
    "\n",
    "# Top and bottom indices along z that contain sample for all scans (after rotation /!\\)\n",
    "ind_all_top = 125\n",
    "ind_all_bottom = 510\n",
    "\n",
    "threshold = 0.10 # Linear attenuation threshold [/cm], gas to wood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters - Run 21 (Oak, low heat)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Rotation\n",
    "rot_coronal_Fiji = -1.4 # [Deg]\n",
    "rot_sagittal_Fiji = -5.0 # [Deg]\n",
    "rot_z_Fiji = -169.0 # [Deg] This angle: such that tree growth is vertical from bottom to top in Fiji, after sagittal / coronal rotation\n",
    "\n",
    "# Top and bottom indices along z that contain sample for all scans (after rotation /!\\)\n",
    "ind_all_top = 140\n",
    "ind_all_bottom = 470\n",
    "\n",
    "threshold = 0.065 # Linear attenuation threshold [/cm], gas to wood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters - Run 23 (Walnut, low heat)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Rotation\n",
    "rot_coronal_Fiji = -1.0 # [Deg]\n",
    "rot_sagittal_Fiji = -5.0 # [Deg]\n",
    "rot_z_Fiji = -91.5 # [Deg] This angle: such that tree growth is vertical from bottom to top in Fiji, after sagittal / coronal rotation\n",
    "\n",
    "# Top and bottom indices along z that contain sample for all scans (after rotation /!\\)\n",
    "ind_all_top = 130\n",
    "ind_all_bottom = 465\n",
    "\n",
    "threshold = 0.05 # Linear attenuation threshold [/cm], gas to wood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters - Run 24 (Birch, low heat)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Rotation\n",
    "rot_coronal_Fiji = -0.2 # [Deg]\n",
    "rot_sagittal_Fiji = 0.5 # [Deg]\n",
    "rot_z_Fiji = 40.0 # [Deg] This angle: such that tree growth is vertical from bottom to top in Fiji, after sagittal / coronal rotation\n",
    "\n",
    "# Top and bottom indices along z that contain sample for all scans (after rotation /!\\)\n",
    "ind_all_top = 170\n",
    "ind_all_bottom = 480\n",
    "\n",
    "threshold = 0.05 # Linear attenuation threshold [/cm], gas to wood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters - Run 26 (Birch, high heat)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running scan #27 - 16b\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ind_scan = 26\n",
    "this_scan = list_scans[ind_scan]\n",
    "print('Running scan #'+str(ind_scan+1)+' - '+this_scan.folder_name)\n",
    "\n",
    "# Re-load scan data\n",
    "with open(path_save+case+'/post_pro_data.pickle', 'rb') as handle:\n",
    "    list_scans = pickle.load(handle)\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Check list\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running scan #1/40 - 16b\n",
      "\tLoading data took: 23.302690505981445\n",
      "\tRotating data took: 10.08382248878479\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 165, ind_bottom: 450, height: 286\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.445164442062378\n",
      "\tStarting 2D pore analysis ....... Done, it took: 44.405805826187134\n",
      "\tSkeletonize and analysis took: 63.365785360336304\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 321.6662292480469\n",
      "Running scan #2/40 - 17a\n",
      "\tLoading data took: 27.724085807800293\n",
      "\tRotating data took: 10.212224006652832\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 166, ind_bottom: 449, height: 284\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.291387796401978\n",
      "\tStarting 2D pore analysis ....... Done, it took: 37.477680921554565\n",
      "\tSkeletonize and analysis took: 68.42950248718262\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 325.5451946258545\n",
      "Running scan #3/40 - 17b\n",
      "\tLoading data took: 36.51749229431152\n",
      "\tRotating data took: 10.09068250656128\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 167, ind_bottom: 448, height: 282\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.400534629821777\n",
      "\tStarting 2D pore analysis ....... Done, it took: 37.33538627624512\n",
      "\tSkeletonize and analysis took: 60.04542255401611\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 329.1658880710602\n",
      "Running scan #4/40 - 18a\n",
      "\tLoading data took: 37.21260452270508\n",
      "\tRotating data took: 10.040129899978638\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 168, ind_bottom: 447, height: 280\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.41597604751587\n",
      "\tStarting 2D pore analysis ....... Done, it took: 38.73234820365906\n",
      "\tSkeletonize and analysis took: 61.941654205322266\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 320.1652874946594\n",
      "Running scan #5/40 - 18b\n",
      "\tLoading data took: 28.52612566947937\n",
      "\tRotating data took: 9.984466314315796\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 169, ind_bottom: 446, height: 278\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.406113624572754\n",
      "\tStarting 2D pore analysis ....... Done, it took: 36.043243169784546\n",
      "\tSkeletonize and analysis took: 60.62600016593933\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 306.2815957069397\n",
      "Running scan #6/40 - 19a\n",
      "\tLoading data took: 29.275547742843628\n",
      "\tRotating data took: 9.997173309326172\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 170, ind_bottom: 445, height: 276\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.429179668426514\n",
      "\tStarting 2D pore analysis ....... Done, it took: 35.98790740966797\n",
      "\tSkeletonize and analysis took: 63.475600481033325\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 319.79433822631836\n",
      "Running scan #7/40 - 19b\n",
      "\tLoading data took: 43.9714241027832\n",
      "\tRotating data took: 10.638330459594727\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 171, ind_bottom: 445, height: 275\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.184038639068604\n",
      "\tStarting 2D pore analysis ....... Done, it took: 36.22511863708496\n",
      "\tSkeletonize and analysis took: 80.35739040374756\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 354.15928196907043\n",
      "Running scan #8/40 - 20a\n",
      "\tLoading data took: 31.581090211868286\n",
      "\tRotating data took: 20.54261827468872\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 442, height: 271\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.179278135299683\n",
      "\tStarting 2D pore analysis ....... Done, it took: 35.12991666793823\n",
      "\tSkeletonize and analysis took: 56.66677212715149\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 312.5612828731537\n",
      "Running scan #9/40 - 20b\n",
      "\tLoading data took: 28.418042182922363\n",
      "\tRotating data took: 10.130267858505249\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 171, ind_bottom: 442, height: 272\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.479499101638794\n",
      "\tStarting 2D pore analysis ....... Done, it took: 31.915594816207886\n",
      "\tSkeletonize and analysis took: 57.66382670402527\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 309.72969245910645\n",
      "Running scan #10/40 - 21a\n",
      "\tLoading data took: 40.4517343044281\n",
      "\tRotating data took: 10.069512367248535\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 441, height: 270\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.524111270904541\n",
      "\tStarting 2D pore analysis ....... Done, it took: 36.312477111816406\n",
      "\tSkeletonize and analysis took: 73.50552272796631\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 342.63240122795105\n",
      "Running scan #11/40 - 21b\n",
      "\tLoading data took: 29.53617262840271\n",
      "\tRotating data took: 15.865144491195679\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 441, height: 270\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.626570701599121\n",
      "\tStarting 2D pore analysis ....... Done, it took: 35.48144817352295\n",
      "\tSkeletonize and analysis took: 62.415823221206665\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 323.5709090232849\n",
      "Running scan #12/40 - 22\n",
      "\tLoading data took: 27.526840925216675\n",
      "\tRotating data took: 10.621512413024902\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 441, height: 270\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.538973331451416\n",
      "\tStarting 2D pore analysis ....... Done, it took: 35.939942359924316\n",
      "\tSkeletonize and analysis took: 60.6863112449646\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 315.9940242767334\n",
      "Running scan #13/40 - 23\n",
      "\tLoading data took: 27.421972513198853\n",
      "\tRotating data took: 17.03966498374939\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 440, height: 269\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.387789964675903\n",
      "\tStarting 2D pore analysis ....... Done, it took: 36.00000715255737\n",
      "\tSkeletonize and analysis took: 55.09887337684631\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 315.8354289531708\n",
      "Running scan #14/40 - 24\n",
      "\tLoading data took: 35.77847170829773\n",
      "\tRotating data took: 10.653706073760986\n",
      "\t3D smoothing for thresholding of mask_solid and mask_needle:\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t\tDetected ROI:\tind_top: 172, ind_bottom: 441, height: 270\n",
      "\tClosing mask solid\n",
      "\tCleaning up mask solid (3D opening), isolating largest particle\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\t Processing 18 chunks of size [349 426 426] .................. Done\n",
      "\tSaving mask_solid and mask_solid_closed:\n",
      "\tParticle identification and analysis took: 10.21798324584961\n",
      "\tStarting 2D pore analysis ....... Done, it took: 35.175267696380615\n",
      "\tSkeletonize and analysis took: 59.94408559799194\n",
      "\tUpdating save pickle\n",
      "\tThis scan took: 313.08272433280945\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "##%% Main loop: run over all scans\n",
    "\n",
    "# Re-load scan data\n",
    "with open(path_save+case+'/post_pro_data.pickle', 'rb') as handle:\n",
    "    list_scans = pickle.load(handle)\n",
    "\n",
    "nb_scans = len(list_scans)\n",
    "force_redo = True\n",
    "\n",
    "for ind_scan,this_scan in enumerate(list_scans[26:]):\n",
    "    tic_scan = time.time()\n",
    "\n",
    "    print('Running scan #'+str(ind_scan+1)+'/'+str(nb_scans)+' - '+this_scan.folder_name)\n",
    "\n",
    "    # Loading data\n",
    "    tic = time.time()\n",
    "    data = File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic/').readAll()\n",
    "    print('\\tLoading data took: '+str(time.time()-tic))\n",
    "\n",
    "    # Rotating data\n",
    "    tic = time.time()\n",
    "    data = rotate_data(data, rot_coronal_Fiji, rot_sagittal_Fiji, rot_z_Fiji)\n",
    "    print('\\tRotating data took: '+str(time.time()-tic))\n",
    "    File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic_rotatedZ/').saveTiff(data[data.shape[0]//2-100,:,:], ind=0)\n",
    "    File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic_rotatedZ/').saveTiff(data[data.shape[0]//2,:,:], ind=1)\n",
    "    File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic_rotatedZ/').saveTiff(data[data.shape[0]//2+100,:,:], ind=2)\n",
    "    File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic_rotatedXY/').saveTiff(data[:,data.shape[1]//2,:], ind=0)\n",
    "    File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic_rotatedXY/').saveTiff(data[:,:,data.shape[2]//2], ind=1)\n",
    "\n",
    "    # Computing/Loading masks solid\n",
    "    this_scan.ind_all_top = ind_all_top\n",
    "    this_scan.ind_all_bottom = ind_all_bottom\n",
    "    this_scan, mask_solid, mask_solid_closed = compute_mask_solid(this_scan, data, threshold, force_redo = force_redo)\n",
    "\n",
    "    # Compute mean solid attenuation and solid volume before cropping\n",
    "    this_scan.mu_solid_not_cropped_open, this_scan.volume_solid_not_cropped_open, this_scan.volume_solid_not_cropped_open_m3 = mean_attenuation_volume(this_scan, mask_solid, data)\n",
    "    this_scan.mu_solid_not_cropped_closed, this_scan.volume_solid_not_cropped_closed, this_scan.volume_solid_not_cropped_closed_m3 = mean_attenuation_volume(this_scan, mask_solid_closed, data)\n",
    "    this_scan.porosity_not_cropped = 1 - np.sum(mask_solid.flatten()) / np.sum(mask_solid_closed.flatten())\n",
    "\n",
    "    # Crop ROI in mask_solid and mask_solid_closed\n",
    "    mask_solid = mask_crop_ROI(mask_solid, this_scan.ind_top, this_scan.ind_bottom)\n",
    "    mask_solid_closed = mask_crop_ROI(mask_solid_closed, this_scan.ind_top, this_scan.ind_bottom)\n",
    "\n",
    "    # Compute mean solid attenuation and solid volume after cropping\n",
    "    this_scan.mu_solid_open, this_scan.volume_solid_open, this_scan.volume_solid_open_m3 = mean_attenuation_volume(this_scan, mask_solid, data)\n",
    "    this_scan.mu_solid_closed, this_scan.volume_solid_closed, this_scan.volume_solid_closed_m3 = mean_attenuation_volume(this_scan, mask_solid_closed, data)\n",
    "    this_scan.porosity = 1 - np.sum(mask_solid.flatten()) / np.sum(mask_solid_closed.flatten())\n",
    "\n",
    "    # Analyze pore structure (3D)\n",
    "    mask_pores = mask_solid_closed * (~mask_solid)\n",
    "    tic = time.time()\n",
    "    lab = measure.label(mask_pores)\n",
    "    properties = np.array(measure.regionprops(lab))\n",
    "    print('\\tParticle identification and analysis took: '+str(time.time()-tic))\n",
    "\n",
    "    # Cleaning up pores (3D)\n",
    "    properties = compute_height(properties)\n",
    "    areas = np.array([prop.area for prop in properties])\n",
    "    height = np.array([prop.height for prop in properties])\n",
    "    this_scan.ind_properties_cleaned = np.where((areas>500) * (areas < 300000) * (height > 100))[0]\n",
    "    properties_cleaned = properties[this_scan.ind_properties_cleaned]\n",
    "\n",
    "    # Compute equivalent cylinder diameters (3D)\n",
    "    properties_cleaned = compute_equivalent_cylinder_diameters(properties_cleaned)\n",
    "    this_scan.nb_pores = len(properties_cleaned)\n",
    "    # this_scan.pore_properties = properties_cleaned # Don't save this, otherwise pickle too heavy (image data stored)\n",
    "\n",
    "    # Analyze pore structure (2D, slice by slice)\n",
    "    print('\\tStarting 2D pore analysis ', end ='')\n",
    "    tic = time.time()\n",
    "    this_scan = analyze_pores_2d(this_scan, mask_pores)\n",
    "    print(' Done, it took: '+str(time.time()-tic))\n",
    "\n",
    "    # Build and analyze skeleton for tortuosity\n",
    "    tic = time.time()\n",
    "    skeleton = compute_skeleton(this_scan, mask_pores, force_redo = force_redo)\n",
    "    skeleton_lab = measure.label(skeleton)\n",
    "    skeleton_properties = measure.regionprops(skeleton_lab)\n",
    "    print('\\tSkeletonize and analysis took: '+str(time.time()-tic))\n",
    "\n",
    "    # Extract which pores to keep from skeleton\n",
    "    skeleton_properties = compute_height(skeleton_properties)\n",
    "    skeleton_areas = np.array([prop.area for prop in skeleton_properties])\n",
    "    skeleton_height = np.array([prop.height for prop in skeleton_properties])\n",
    "    this_scan.skeleton_ind_properties_cleaned = np.where((skeleton_areas>50) * (skeleton_areas < 1000) * (skeleton_height > 100))[0]\n",
    "\n",
    "    # Compute tortuosity for each pore\n",
    "    skeleton_properties_cleaned = compute_tortuosity(np.array(skeleton_properties)[this_scan.skeleton_ind_properties_cleaned])\n",
    "\n",
    "    # Save relevant pore & skeleton properties\n",
    "    this_scan.areas = [prop.area for prop in properties_cleaned]\n",
    "    this_scan.equivalent_cylinder_diameters = [prop.equivalent_cylinder_diameter for prop in properties_cleaned]\n",
    "    this_scan.heights = [prop.height for prop in properties_cleaned]\n",
    "    this_scan.skeleton_pore_path_length = [prop.length_skeleton for prop in skeleton_properties_cleaned]\n",
    "    this_scan.tortuosity = [prop.tortuosity for prop in skeleton_properties_cleaned if not np.isnan(prop.tortuosity) and prop.tortuosity > 1]\n",
    "\n",
    "    # Update save of scan data\n",
    "    print('\\tUpdating save pickle')\n",
    "    with open(path_save+case+'/post_pro_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(list_scans, handle)\n",
    "\n",
    "    print('\\tThis scan took: '+str(time.time()-tic_scan))\n",
    "print('Done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "with open(path_save+case+'/post_pro_data.pickle', 'rb') as handle:\n",
    "    list_scans = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load scan data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "34\n",
      "20b\n",
      "0.21614414068577625\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(list_scans))\n",
    "\n",
    "ind_scan = 34\n",
    "\n",
    "print(list_scans[ind_scan].ind)\n",
    "print(list_scans[ind_scan].folder_name)\n",
    "print(list_scans[ind_scan].porosity)\n",
    "print(list_scans[ind_scan].nb_pores)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading data took: 37.67014169692993\n",
      "Loading scan #1/41 - 23\n"
     ]
    }
   ],
   "source": [
    "ind_slice_X = 600-1 # Reslice from Top in Fiji\n",
    "ind_slice_Y = 620-1 # Reslice from Left in Fiji\n",
    "nb_scans = len(list_scans)\n",
    "\n",
    "for ind_scan,this_scan in enumerate(list_scans[-2:]):\n",
    "\n",
    "    print('Loading scan #'+str(ind_scan+1)+'/'+str(nb_scans)+' - '+this_scan.folder_name)\n",
    "\n",
    "    # Loading data\n",
    "    tic = time.time()\n",
    "    data = File(path_save+case+'/'+this_scan.folder_name+'/b_movingRegisteredToStatic/').readAll()\n",
    "    print('\\tLoading data took: '+str(time.time()-tic))\n",
    "\n",
    "    File(path_save+case+'/probe_sliceX_'+str(ind_slice_X).zfill(4)).saveTiff(data[:,ind_slice_X,:], ind = this_scan.ind)\n",
    "    File(path_save+case+'/probe_sliceY_'+str(ind_slice_Y).zfill(4)).saveTiff(data[:,:,ind_slice_Y], ind = this_scan.ind)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Export vertical slices\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658937477.8635976\n"
     ]
    }
   ],
   "source": [
    "print(time.time())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "02\n",
      "03\n",
      "04a\n",
      "04b\n",
      "05a\n",
      "05b\n",
      "06a\n",
      "06b\n",
      "07a\n",
      "07b\n",
      "08a\n",
      "08b\n",
      "09a\n",
      "09b\n",
      "10a\n",
      "10b\n",
      "11\n",
      "12\n",
      "13a\n",
      "13b\n",
      "14a\n",
      "14b\n",
      "15a\n",
      "15b\n",
      "16a\n",
      "16b\n",
      "17a\n",
      "17b\n",
      "18a\n",
      "18b\n",
      "19a\n",
      "19b\n",
      "20a\n",
      "20b\n",
      "21a\n",
      "21b\n",
      "22\n",
      "23\n",
      "24\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/e/eboigne/local/miniconda3/envs/als/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: Mean of empty slice\n",
      "  if sys.path[0] == \"\":\n"
     ]
    }
   ],
   "source": [
    "for scan in list_scans[:]:\n",
    "    scan.voxel_size = voxel_size\n",
    "\n",
    "    # 2D slice properties\n",
    "    scan.slice_ind_keep = np.concatenate(scan.slice_eq_diameters) > 2\n",
    "    scan.slice_mean_eq_diameter = np.nanmean(np.concatenate(scan.slice_eq_diameters)[scan.slice_ind_keep]) * voxel_size\n",
    "    scan.slice_mean_eccentricity = np.nanmean(np.concatenate(scan.slice_ellipse_eccentricity)[scan.slice_ind_keep])\n",
    "    scan.slice_mean_angle_to_growth = np.nanmean(np.concatenate(scan.slice_angles_to_growth)[scan.slice_ind_keep])\n",
    "\n",
    "    # 3D pore/skeleton properties\n",
    "    scan.mean_eq_diameter = np.nanmean(scan.equivalent_cylinder_diameters)\n",
    "    scan.mean_tortuosity = np.nanmean(scan.tortuosity)\n",
    "\n",
    "    # 3D single particle properties, assuming a cylindrical shape\n",
    "    scan.total_sample_height = scan.ind_bottom+scan.ind_margin - (scan.ind_top-scan.ind_margin)\n",
    "    scan.total_sample_height_microns = scan.total_sample_height * voxel_size\n",
    "    scan.total_sample_eq_diameter = np.sqrt(4.0 / np.pi * scan.volume_solid_not_cropped_closed / scan.total_sample_height)\n",
    "    scan.total_sample_eq_diameter_microns = scan.total_sample_eq_diameter * voxel_size\n",
    "\n",
    "    print(scan.folder_name)\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Faster post-pro\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUpdating save pickle\n"
     ]
    }
   ],
   "source": [
    "print('\\tUpdating save pickle')\n",
    "with open(path_save+case+'/post_pro_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(list_scans, handle)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Update save of scan data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.143631982946165\n"
     ]
    }
   ],
   "source": [
    "print(list_scans[0].slice_)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}